import os
import json
import uuid
import glob
from datetime import datetime
from typing import TypedDict, List
from dotenv import load_dotenv
import streamlit as st

# LangChain & LangGraph Imports
from langgraph.graph import StateGraph, END
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document

# Models
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from sentence_transformers import SentenceTransformer

# -------------------------
# 1. TEMEL AYARLAR
# -------------------------
load_dotenv()
st.set_page_config(
    page_title="âŠ¹ à£ª ï¹ğ“Šï¹ğ“‚ï¹âŠ¹ à£ª Ë–",
    page_icon="ğŸ´â€â˜ ï¸",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# -------------------------
# Ã–zel TasarÄ±m (Gemini/ChatGPT TarzÄ±)
# -------------------------
st.markdown("""
<style>
    /* Ana baÅŸlÄ±k rengi */
    h1 { color: #FF4B4B; }
    
    /* Chat mesajlarÄ± Ã§erÃ§evesi */
    .stChatMessage {
        border: 1px solid #333; /* Hafif Ã§erÃ§eve */
        border-radius: 12px;
        padding: 15px;
    }

    /* --- SIDEBAR TASARIMI --- */
    
    /* Sidebar'daki "Secondary" butonlarÄ± (GeÃ§miÅŸ sohbetler) ÅŸeffaf yap */
    section[data-testid="stSidebar"] .stButton button[kind="secondary"] {
        background-color: transparent;
        border: none;
        text-align: left; /* YazÄ±yÄ± sola yasla */
        width: 100%;
        color: inherit; /* Temaya uygun renk */
        padding: 10px;
        transition: all 0.2s ease; /* YumuÅŸak geÃ§iÅŸ */
    }

    /* Ãœzerine gelince (Hover) hafif gri/beyaz olsun */
    section[data-testid="stSidebar"] .stButton button[kind="secondary"]:hover {
        background-color: rgba(255, 255, 255, 0.1); /* Hafif aydÄ±nlatma */
        padding-left: 15px; /* Hafif saÄŸa kayma efekti */
        color: #FF4B4B;
    }

    /* "Yeni Sohbet" butonu (Primary) dikkat Ã§ekici kalsÄ±n */
    section[data-testid="stSidebar"] .stButton button[kind="primary"] {
        width: 100%;
        border-radius: 20px;
        font-weight: bold;
    }
    
    /* Sidebar baÅŸlÄ±klarÄ±nÄ± biraz kÃ¼Ã§Ã¼ltelim */
    section[data-testid="stSidebar"] h1, section[data-testid="stSidebar"] h2, section[data-testid="stSidebar"] h3 {
        font-size: 1rem;
        text-transform: uppercase;
        letter-spacing: 1px;
        opacity: 0.7;
    }
</style>
""", unsafe_allow_html=True)

DEFAULT_PDF = "one_piece.pdf"
HISTORY_FOLDER = "chat_history"

# KlasÃ¶r yoksa oluÅŸtur
if not os.path.exists(HISTORY_FOLDER):
    os.makedirs(HISTORY_FOLDER)

# API Keys
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
has_gemini = bool(GOOGLE_API_KEY)
has_openai = bool(OPENAI_API_KEY)

# -------------------------
# 2. YARDIMCI FONKSÄ°YONLAR (Storage & PDF)
# -------------------------
def save_chat_history(session_id, messages):
    """Sohbeti JSON olarak kaydeder"""
    filepath = os.path.join(HISTORY_FOLDER, f"{session_id}.json")
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(messages, f, ensure_ascii=False, indent=4)

def load_chat_history(session_id):
    """JSON'dan sohbeti yÃ¼kler"""
    filepath = os.path.join(HISTORY_FOLDER, f"{session_id}.json")
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def get_all_chats():
    """TÃ¼m kayÄ±tlÄ± sohbetleri listeler ve ilk mesajÄ± baÅŸlÄ±k yapar"""
    files = glob.glob(os.path.join(HISTORY_FOLDER, "*.json"))
    chats = []
    for f in files:
        filename = os.path.basename(f).replace(".json", "")
        timestamp = os.path.getctime(f)
        date_str = datetime.fromtimestamp(timestamp).strftime('%d.%m %H:%M')
        
        # DosyanÄ±n iÃ§ini oku ve ilk mesajÄ± al
        try:
            with open(f, "r", encoding="utf-8") as file:
                data = json.load(file)
                # Ä°lk kullanÄ±cÄ± mesajÄ±nÄ± bul
                first_msg = next((m["content"] for m in data if m["role"] == "user"), "Yeni Sohbet")
                # Ã‡ok uzunsa kÄ±salt (30 karakter)
                title = (first_msg[:25] + '..') if len(first_msg) > 25 else first_msg
        except:
            title = "Yeni Sohbet"

        chats.append({"id": filename, "date": date_str, "title": title})
    
    # En yeniden eskiye sÄ±rala
    chats.sort(key=lambda x: x["date"], reverse=True)
    return chats

def format_docs_for_prompt(docs) -> str:
    parts = []
    for d in docs:
        page = d.metadata.get("page", "?")
        parts.append(f"[Sayfa {page}] {d.page_content}")
    return "\n\n".join(parts)

def format_history_for_prompt(messages) -> str:
    """Mesaj listesini LLM'in anlayacaÄŸÄ± metne Ã§evirir"""
    formatted = ""
    for msg in messages:
        role = "KullanÄ±cÄ±" if msg["role"] == "user" else "Asistan"
        formatted += f"{role}: {msg['content']}\n"
    return formatted

# -------------------------
# 3. EMBEDDINGS & RETRIEVER (Local)
# -------------------------
class LocalSentenceTransformerEmbeddings(Embeddings):
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, texts):
        vectors = self.model.encode(list(texts), normalize_embeddings=True)
        return [v.tolist() for v in vectors]

    def embed_query(self, text):
        v = self.model.encode([text], normalize_embeddings=True)[0]
        return v.tolist()

@st.cache_resource
def get_embeddings():
    return LocalSentenceTransformerEmbeddings("sentence-transformers/all-MiniLM-L6-v2")

@st.cache_resource
def build_retriever(pdf_path: str):
    if not os.path.exists(pdf_path):
        return None
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = splitter.split_documents(docs)
    embeddings = get_embeddings()
    vectorstore = Chroma.from_documents(splits, embedding=embeddings)
    return vectorstore.as_retriever(search_kwargs={"k": 4})

# Retriever'Ä± baÅŸlat
retriever = build_retriever(DEFAULT_PDF)

# -------------------------
# 4. SIDEBAR & SESSION MANAGEMENT
# -------------------------
st.sidebar.title("ğŸ—‚ï¸ Sohbet GeÃ§miÅŸi")

# Session ID KontrolÃ¼
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
    st.session_state.messages = []

# Yeni Sohbet Butonu
if st.sidebar.button("ï¹ğ“Šï¹ Yeni Sohbet BaÅŸlat", type="primary"):
    st.session_state.session_id = str(uuid.uuid4())
    st.session_state.messages = []
    st.rerun()

st.sidebar.divider()

# Eski Sohbetleri Listele
previous_chats = get_all_chats()

# EÄŸer hiÃ§ sohbet yoksa bilgi ver
if not previous_chats:
    st.sidebar.caption("HenÃ¼z geÃ§miÅŸ sohbet yok.")

for chat in previous_chats:
    # Butonun Ã¼zerinde artÄ±k "Luffy kimdir?" gibi baÅŸlÄ±k yazacak
    # AltÄ±na da kÃ¼Ã§Ã¼k tarih ekliyoruz
    label = f"{chat['title']}" 
    
    # kind="secondary" diyerek CSS'in bunu yakalamasÄ±nÄ± saÄŸlÄ±yoruz
    if st.sidebar.button(label, key=chat['id'], use_container_width=True, type="secondary"):
        st.session_state.session_id = chat['id']
        st.session_state.messages = load_chat_history(chat['id'])
        st.rerun()

# Ayarlar
st.sidebar.divider()
st.sidebar.subheader("âš™ï¸ Ayarlar")
model_secimi = st.sidebar.radio("Model:", ("Gemini 3 Flash Preview", "OpenAI GPT-3.5 Turbo"))
show_sources = st.sidebar.toggle("KaynaklarÄ± gÃ¶ster", value=True)

# LLM SeÃ§imi
llm = None
if model_secimi == "Gemini 3 Flash Preview":
    if not has_gemini:
        st.error("Gemini API Key eksik!")
        st.stop()
    llm = ChatGoogleGenerativeAI(model="gemini-3-flash-preview", temperature=0)
else:
    if not has_openai:
        st.error("OpenAI API Key eksik!")
        st.stop()
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# -------------------------
# 5. LANGGRAPH (MEMORY DESTEKLÄ°)
# -------------------------

# State TanÄ±mÄ± (ArtÄ±k history de taÅŸÄ±yor)
class GraphState(TypedDict):
    question: str
    generation: str
    documents: List[Document]
    chat_history: str  # <--- YENÄ°: GeÃ§miÅŸ sohbet metni

def retrieve(state):
    """Belgeleri bulur"""
    print("---RETRIEVE---")
    question = state["question"]
    if retriever:
        documents = retriever.invoke(question)
    else:
        documents = []
    return {"documents": documents, "question": question}

def generate(state):
    """CevabÄ± Ã¼retir"""
    print("---GENERATE---")

    if llm is None:
        return {"generation": "Hata: Bir yapay zeka modeli seÃ§ilmedi veya API anahtarÄ± eksik."}
    question = state["question"]
    documents = state["documents"]
    chat_history = state["chat_history"] # GeÃ§miÅŸi al
    
    # Prompt - ArtÄ±k hafÄ±zasÄ± var!
    system_prompt = (
        "Sen 'Going-Chaty' One Piece evrenine hakim, neÅŸeli ve yardÄ±msever bir asistansÄ±n.\n"
        "GÃ¶revlerin ÅŸunlar:\n\n"
        "1. **SOHBET VE YORUM:** EÄŸer kullanÄ±cÄ± senin fikrini sorarsa (Ã–rn: 'Hangi meyveyi istersin?', 'En sevdiÄŸin karakter kim?'), "
        "baÄŸlama baÄŸlÄ± kalmak zorunda deÄŸilsin :). YaratÄ±cÄ±, eÄŸlenceli ve bir One Piece hayranÄ± gibi cevap ver. "
        "(Ã–rn: 'Gomu Gomu no Mi isterdim Ã§Ã¼nkÃ¼ uÃ§mak Ã§ok havalÄ±!' gibi).\n\n"
        "2. **BÄ°LGÄ° SORULARI:** EÄŸer kullanÄ±cÄ± dokÃ¼manla ilgili teknik veya bilgi iÃ§erikli bir soru sorarsa, "
        "cevabÄ± SADECE aÅŸaÄŸÄ±daki BAÄLAM (Context) bilgisini kullanarak ver.\n\n"
        "3. **BÄ°LÄ°NMEYEN BÄ°LGÄ°:** EÄŸer sorulan *bilgi* baÄŸlamda yoksa dÃ¼rÃ¼stÃ§e 'Bu detay dokÃ¼manlarda geÃ§miyor ama istersen seninle teoriler Ã¼zerine konuÅŸabiliriz!' de.\n\n"
        "SOHBET GEÃ‡MÄ°ÅÄ°:\n{chat_history}\n\n"
        "BAÄLAM:\n{context}"
    )
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])
    
    rag_chain = prompt | llm | StrOutputParser()
    
    context_text = format_docs_for_prompt(documents)
    
    # Zinciri Ã§alÄ±ÅŸtÄ±r
    generation = rag_chain.invoke({
        "context": context_text, 
        "chat_history": chat_history, 
        "input": question
    })
    
    return {"generation": generation}

# Graph OluÅŸturma
workflow = StateGraph(GraphState)
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate)
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)
app_graph = workflow.compile()

# -------------------------
# 6. ARAYÃœZ (CHAT UI)
# -------------------------
st.title("Going-Chaty ğŸ‘’ğŸ–ğŸ´â€â˜ ï¸ğŸˆâ˜€ï¸")

# MesajlarÄ± Ekrana Yaz
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# KullanÄ±cÄ± Girdisi
user_input = st.chat_input("Sorunuzu yazÄ±n...")

if user_input:
    # 1. KullanÄ±cÄ± mesajÄ±nÄ± ekle ve gÃ¶ster
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)
    
    # 2. GeÃ§miÅŸi metne Ã§evir (LLM iÃ§in)
    # Son mesajÄ± hariÃ§ tutuyoruz ki tekrar etmesin (zaten input olarak gidiyor)
    history_text = format_history_for_prompt(st.session_state.messages[:-1])

    with st.chat_message("assistant"):
        try:
            inputs: GraphState = {
                "question": user_input,
                "documents": [],
                "generation": "",
                "chat_history": history_text # <--- GeÃ§miÅŸi gÃ¶nderiyoruz
            }
            
            result = app_graph.invoke(inputs)
            
            answer = result["generation"]
            source_docs = result["documents"]
            
            st.write(answer)
            
            # 3. CevabÄ± kaydet
            st.session_state.messages.append({"role": "assistant", "content": answer})
            
            # 4. Dosyaya KalÄ±cÄ± Olarak Kaydet (JSON)
            save_chat_history(st.session_state.session_id, st.session_state.messages)

            if show_sources and source_docs:
                with st.expander("ğŸ“š Kaynaklar"):
                    for i, d in enumerate(source_docs, 1):
                        st.markdown(f"**{i}.** {d.page_content[:200]}...")

        except Exception as e:
            st.error(f"Hata oluÅŸtu: {e}")